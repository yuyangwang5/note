# A Yoke of Oxen and a Thousand Chickens for Heavy Lifting Graph Processing

## 2. GRAPH PROCESSING ON HETEROGENEOUS ARCHITECTURES: OPPORTUNITY AND CHALLENGES，异构架构上，图处理的机遇与挑战

机遇：
* **GPU多线程优势**
* **CPU和GPU不同架构，可充分利用其优势**  
  CPU适合顺序处理（适用于少数的高度数节点），GPU适合并行处理（适用于低度数节点）。

挑战：
* **内/显存需求大**
* **应使应用程序符合SIMD计算模型**  
* **高层次抽象到API映射如何减少效率损失**

## 3. PERFORMANCE MODEL，性能模型

该模型用于解答，相比于仅在CPU上计算，异构计算是否更有益。设$P={P_{cpu}, P_{acc}}$为异构架构，包含一个CPU和一个加速器两个处理节点。作出如下假设:
1. 每个处理节点都有自己本地内存。处理元素双向连通，通信速率计量为每秒传输的边数（E/s）；
2. 处理模型为批量同步并行（BSP）；
3. $P_{cpu}$内存足够装入并处理完整图；
4. $P_{acc}$处理图的速率快于$P_{cpu}$；
5. $P_{cpu}$负责调度工作负载、收集计算结果。模型假设这些开销与算法处理时间相比可以忽略不记。

设图为$G=(V,E)$，在处理器p上处理G的一个分区$G_p = (V_p, E_p)$，时间计量公式为：

$$t(G_p) =  \frac{|E_{p}^{b}|}{c} + \frac{|E_{p}|}{r_p} \tag{1}$$

其中，$r_p$代表p处理边的速度(E/s)，$E_{p}^{b} \subseteq E_{p}$ 表示边界边（即该边的src或dst没有存储在p中）。c是传输速度。  
此处存储思想应该是GPU存储一部分图进行计算，CPU存储一部分，分配完就不会再变了。所以计算过程没有关于整个图的换入换出。

![Graph partitioning on a heterogeneous node](./pic/2012%20TOTEM/0%20Graph%20partitioning%20on%20a%20heterogeneous%20node.png "Graph partitioning on a heterogeneous node")

方程1中两个分式分别代表更新边界边（通信阶段）和处理器p上处理该分区的边（计算阶段）的时间总和。

设makespan为一轮迭代处理完图所需要的时间，有：

$$m_p(G) = \max_{p\in P}(t(G_p)) \tag{2}$$

根据我们定义，有CPU运算速度最慢。我们定义对异构平台加速比的评估：

$$S_p(G) = \frac{m_{\{cpu\}}(G)}{m_P (G)} = \frac{m_{\{cpu\}}(G)}{m_{\{cpu\}}(G_{cpu})} = \frac{|E|/r_{cpu}}{|E_{cpu}^{b}|/c + |E_{cpu}| / r_{cpu}} \tag{3}$$ 

定义$\alpha$为分配到CPU的边在所有的边中的占比，而$\beta$为CPU存储的边中边界边在全局所有边中的占比，有：


$$ S_p(G) = \frac{|E|/r_{cpu}}{\beta |E|/c + \alpha |E| / r_{cpu}} = \frac{c}{\beta r_{cpu} + \alpha c} = \frac{1}{\frac{\beta \cdot r_{cpu}}{c} + \alpha} \tag{4}$$

对于c，其正比于传输速率，反比于每条边所需要传输的data成反比。  
$\beta$与图的分割有很大关系。  
$\alpha$的大小是可以调整的，不过收到GPU显存限制。

## TOTEM
TOTEM为用于有多个GPU的异构系统，采用BSP并行计算模型。每个超步中包含三个阶段：计算、通信、同步。同步用于保证消息传递完成再进行下一个超步。

最初，TOTEM将图划分，给每个分区分配一个处理单元。

计算阶段，各个处理单元并行进行计算，处理分配给它的节点。

通信阶段交流边界边的信息。计算时会在先本地缓冲区存储要发送的消息，通信阶段再发送到对应的处理单元。引擎模型会在原处理单元上，对要发送到相同顶点的消息进行聚合处理，以减小通信开销。同步阶段隐式作为通信阶段的一部分。

当所有处理节点都投票该终止计算时，引擎终止执行。之后会收集所有分局计算结果并合并。

图采用CSR格式进行存储，如图所示：
![CSR](./pic/2012%20TOTEM/1%20CSR.png "CSR")

对于每个分区，顶点ID进行了重新编号，范围为$0 \sim |V_p|-1$，该ID配上分区ID来对所有定点进行编号索引。

E中存储目的节点，存储信息包括分区的id。对于边界边，存储信息会有所不同，不是远程邻居的id，二是缓冲区索引。

S表示每个顶点的数据。

访问邻居状态是原子操作。这里采用改进：对E中每个顶点邻居集合进行排序，先处理本地边（访问S），再处理边界边（访问缓冲区）。

之后来看看对边界边的处理。对于每个处理单元，TOTEM会维护两个缓冲区。一个是发送缓冲区，为每一个远程邻居设有一个条目，对应图中为outbox；另一个接收缓冲区则为本地每一个顶点设置一个条目，对应图中为inbox，以此对消息进行聚合。（看源码，了解如何使节点ID和对应缓冲区位置对应）

不过TOTEM有一个限制：在发送缓冲区消息时是将整个缓冲发送出去。可能有些节点在当轮计算得到的值并没有改变，这增加了通信开销。希望能对边界边进行选择性通信。

实验测试了两个算法：BFS和PageRank。由于BFS计算量小于PageRank，所以它对TOTEM引入的开销更为敏感。

实验表明CPU的计算时间为主要瓶颈，通信产生的开销相比于计算非常的低。













