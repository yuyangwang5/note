# Garaph: Efficient GPU-accelerated Graph Processing on a Single Machine with Balanced Replication

## 系统概述
采用CSR和CSC。分图处理，将点分为不相交的顶点集，每个顶点集均有一个shard，shard中存储dst在点集中的边，即点集的入边集合，shard中边按dst排序。

为了提高GPU利用率，允许每个shard小于共享内存（以能放入）。每个shard中的节点数为$C_{shm}/(N_{Block} * S_{vertex})$，其中$C_{shm}$表示共享内存大小，$N_{Block}$表示SM中线程数量，$S_{vertex}$表示节点的大小。若有48KB共享内存，$S_{vertex}$大于32位（以用于十亿规模结点的图）。那么每个分片最多可以包含12K个顶点。由此，我们可以用16位整数表示目标顶点的索引，这样可以节省显存。

为了提高传输速率，可以一次传输多个shard。一次传输的shard集合称为page，希望page可以在GPU中被连续最大存储（多一个shard就存不下）。设$N_s$为流的数量，则shard数目选择，应使得对应的page可以最大连续存储于$1/N_s$的显存中。

系统会采用两种节点中心计算模型，一种是pull，每个顶点拉起去邻近节点状态更新自身；另一种为notify-push（通知-拉取），活跃顶点通知邻居更新状态，邻居再拉取更新自身。CSR只用于通知，Garaph不会在CSR中存储边的值。

![Garaph Architecture](./pic/2017%20Garaph/0%20Garaph%20Architecture.png "Garaph Architecture")

Garaph架构如上图所示，由三个主要部分：调度器 dispatcher，CPU计算内核和GPU计算内核。

* **调度器**  
该模块负责从二级存储中加载图数据，在内存中构建page，并将计算任务分配到CPU和GPU上。
* **GPU/CPU计算内核**  
接收到page后，GPU计算内核会并行处理page的shard，每个shard由一个block处理。为提高效率，GPU端只会使用pull模型（notify-push会导致warp发散）。  
CPU计算内核会将page的边划分为大小相等的集合，每个线程处理一个集合。CPU与GPU之间，任有一个处理完一个page，就会进行一次CPU/GPU同步。

Garaph支持以同步或异步的方式执行。称处理完所有page为一次迭代。同步模型中，每次迭代结束后对顶点数据的更新会被提交，下一次迭代可见；而异步则更改会立即被提交。

容错性考虑，Garagh会定期将顶点写入二级存储。同步运行，每隔n次迭代（用户定义）写入；异步运行有两种处理策略，一是每处理完一个page，就会将更新的顶点数据写入稳定存储；二是每隔n次迭代将所有顶点数据写入稳定存储。发生故障，系统会从二级存储中加载顶点数据继续运行程序。

Garaph实现了GAS(Gather-Apply-Scatter)抽象。若顶点的值发生显著变化，会激活节点。

## GPU图处理

### 图处理引擎

![GPU引擎](./pic/2017%20Garaph/1%20GPU引擎.png "GPU引擎")

GPU在全局内存会维护数组GlobalVertices，该数组存放顶点的值。若有24G全局内存，顶点大小为4字节，则可以存放多大60亿个节点，对于大多数数据集而言是足够的。

shard交给线程块并行处理。当一个page完成计算时，GPU的全局内存和主机内存会同步新的顶点值。现在假设$S_i$为一个目标顶点；
* **初始化**  
GPU在对应SM共享内存中分配数组LocalVertices，里面存储shard中每个顶点的累积值（之后会介绍怎么计算）。线程块中线程使用用户定义的默认顶点值初始化该数组。
* **Gather**    
一个线程处理一条边，其从全局内存获取顶点和变得数据，并增加累积值：$a_u \leftarrow sum(a_u, gather(D_u, D_{(u, v)}, D_v), D_v), v\in Nbr[u]$。为了实现全局内存访问的合并，线程块中连续线程读取全局内存中连续的边数据。
* **Apply**   
每个线程更新共享内存中的顶点值。异步执行时，系统会直接将新的顶点数据提交到GlobalVertices中。
* **GPU到CPU同步**   
一个page处理完毕时，GPU全局内存会和CPU同步。对于异步，GPU全局内存中GlobalVertices的更新值会传输到CPU中存储最新值的数组；对于同步，更新节点时值会存储于GPU临时空间，page计算完后被传输到CPU临时存储空间，在迭代结束后提交。由于PCIe总线是全双工的，且大多数GPU有两个复制引擎，因此同步过程可以与GPU中page处理过程重叠在一起。

### 基于复制的Gather

在Gather阶段，多个边的dst会出现相同的状况，由此会存在写冲突。本文中减少冲突的策略为复制，即对于节点v，在共享内存中存放给R个副本，这R个副本位置连续，R被称为复制因子。  

我们需要将线程分配到不同的副本上，由此需要一个映射公式。设同一个shard $S_i$的顶点具有相同的复制因子$R_i$，对于该分片中任何顶点$u_i$，有

$$addr[u_i^{'}] = (i-r_s)*R_i + tid \% R_i, \forall u_i \in S_i$$

$addr[u_i^{'}]$表示的是对于tid，若它要写入的节点为$u_i$，它所要写入的共享内存的位置。其中$r_s$是$S_i$在共享内存所拥有存储空间的起始索引，可以看出每R个位置归属于一个节点，线程对应于R个位置的哪个，取决于$tid \% R_i$。该过程如下图(a)所示。

![复制节点](./pic/2017%20Garaph/2%20复制节点.png "复制节点")

处理完所有边后，系统需要格外的聚合阶段来聚合所有副本的值。聚合过程如上图(b)所示。由此，对于R的设置，希望$R=2^n$。

下面讨论如何选择复制因子。当复制因子过大时，共享内存中可存储的顶点数会减少。我们需要在冲突程度和空间限制之间取得平衡。

首先我们来估计复制因子$R_i$对$S_i$处理时间$T_G$的影响。设$V_i, E_i$分别为$S_i$中目的节点和边，$t_l$为访问全局内存的时间，$t_a$为共享内存中原子操作耗费的时间。如此，最开始线程从全局内存需要读取$R_i \times |V_i|$次节点数据（初始化共享内存）和$|E_i|$次边数据；进行计算时，共会执行$E_i$次原子操作，由于冲突的存在，每个操作平均需要等待的次数估计为$\frac{|E_i|}{|V_i|\times R_i}$；最后的聚合需要的操作数目为$|V_i|\times t_a \times log R_i$。由此，$T_G(R_i)$可以表示为：

$$T_G(R_i) = R_i|V_i|t_l + |E_i|t_l + \frac{|E_i|^2}{|V_i|R_i}t_a + |V_i|t_a\log{R_i} \tag{1}$$

我们希望$T_G(R_i)$最小。设$\log{R_i} \ll R_i$，则当$R_i|V_i|t_l = \frac{|E_i|^2}{|V_i|R_i}t_a$时$T_G(R_i)$最小。整理可得$R_i = \frac{|E_i|}{V_i} \cdot \sqrt{\frac{t_a}{t_l}}$。在作者实践中，有$t_a \approx t_l$。且当$R_i$为32时，相当于为warp中每一个线程都准备了副本，此时将不存在冲突。由此设定:

$$R_i = 2^{min\{ \lceil \log{\frac{|E_i|}{|V_i|}} - 0.5\rceil, 5\}} \tag{2}$$

## CPU图处理

对于分图，为了使得工作负载均衡，每个子图的边数相同。分图中，边界处节点会被切割，其将被复制，在聚合阶段会将这些值进行聚合。

在CPU的内存中，会维护GlobalVertices数组。处理有两种方式，一是所有边都使用，顶点通过出边拉取邻居状态更新自身，直到收敛或到达一定的迭代次数。状态显著变化的顶点被称为活跃顶点，我们使用位图A表示。

设$n_t$为CPU线程数，page被划分为$n_t$个大小相同的分区，每个线程处理一份分区；若分区第一个顶点和最后一个顶点边界处被切割，则创建一个副本。由此，副本数量不超过$n_t-1$。每个CPU线程维护一份LocalVertices数组，此数组以用户定义的默认值进行初始化。

在Gather阶段，会在LocalVertices数组进行数据聚合。由于副本数量不超过$n_t-1$，数目不大，在所有线程执行完gather阶段后，可以直接使用一个线程进行处理副本处理。在本框架中，线程0会反向扫描所有分区并聚合副本的值。 

在Apply阶段，每个线程进一步更新其LocalVertices数组的顶点值。对于每个分区，如果第一个顶点被复制，它的汇总值在前一个分区最后一个顶点也有出现，因此该顶点会被忽略。在Apply时，同时也会有相应的线程调用activate()函数检查顶点是否活跃，更新位图。

GPU处理完一个page后，会将顶点值发送给CPU内存。接收到新的顶点值时，系统首先会调用Activate()函数更新位图A。若异步运行，系统会直接将它们写入内存的GlobalVertices中，使得来自GPU的更新立即可见。之后系统会将CPU更新的新顶点发回给GPU，覆盖GPU的GlobalVertices数组。若同步运行，系统会将GPU更新的值存于一个临时数组中，每次迭代结束后才提交新值，同时迭代结束后将新GlobalVertices传输给GPU显存中。

在计算上，GPU的顶点更新模式一种为pull模式，每个顶点拉取邻近节点状态执行本地计算。另一种为notify-pull模式，上轮迭代被激活的顶点通知其出边邻居，邻居拉取其入边邻居状态来更新自身。Garaph结合两种模式，将CPU处理扩展为双引擎设计。

假设图可以完全被加载入主存，设$T_{pull}$为pull模式下图处理时间，可见$T_{pull}$与$|V_A|$无关。而notify-pull模式，只有一部分顶点被更新，设$f=|V_A|/|V|$，有近似估计$T_{notify-pull} = 2f T_{pull}$。其中“2”一次来自于通知，一次来自于拉取状态。由此，如果$f \le 1/2$，则采用notify-pull模式。

然而实际情况下，只有部分图可以加载到主存，从辅助存储设备访问出/入边会产生I/O成本。设k为顺序读取和随机读取速度之比（如SSD中$k \approx 10$），notify-pull模式大部分为随机读取，由此有$T_{notify-pull} = 2kf T_{pull}$此时，如果$f \le \frac{1}{2k}$

设$\Gamma = \{u|A[u]=1\}$为上一代活跃顶点的集合，更准确的f估计为$f \approx \frac{\sum_{u\in \Gamma}d_u}{|E|}$。

## 调度器

### CPU-GPU调度
CPU处理时间可以表示为$T_{cpu} = \min \{2fp, 1\}\cdot T_{pull}$，由此在每次迭代开始时，调度器将计算：
$$\alpha = \frac{\min \{2fp, 1\}\cdot T_{pull}}{T_{gpu}}$$

GPU每次计算都要处理page所有边。由此，$T_{pull}/T_{GPU}$主要为CPU/GPU硬件的速度比，其值会在CPU和GPU均处理过page后，通过器处理page的时间更新。

若$\alpha \le 1$，则此迭代可以只使用CPU处理，否则会在CPU和GPU滨兴处理page。混合模式下，一方空闲，系统立马就会分给其一个page。

### GPU多流调度

在主机上，有两个线程在运行：传输线程和计算线程。前者线程将page从主存传输到GPU全局内存，后者线程用于启动GPU内核处理已经传输的page。

利用NVIDIA的Hyper-Q功能，我们使用多流调度来处理CPU-GPU的内存复制和内核执行，使得page处理任务可以分配到多个流上并行执行。在将page处理任务调度到$N_{stream}$上时，传输线程会定期检查哪个流处于空闲状态，并将page任务分配给空闲的流。计算线程定期检查哪个page已经完全传输，并将计算任务分配给相应的流，以出发该page的计算。这种多流调度使得CPU-GPU内存复制和内核执行之间的重叠较高。

## 实验
使用节点复制不仅可以加速page计算，还可以使page间耗费的时间更均衡。下面展示没有使用复制和使用复制策略时，运行PR的时间对比：

![复制节点实验](./pic/2017%20Garaph/3%20复制节点实验.png "复制节点实验")

除此，每个子图采用不同的复制因子，会比采用固定的复制因子效果更好。