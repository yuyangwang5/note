# Garaph: Efficient GPU-accelerated Graph Processing on a Single Machine with Balanced Replication

## 系统概述
采用CSR和CSC。分图处理，将点分为不相交的顶点集，每个顶点集均有一个shard，shard中存储dst在点集中的边，即点集的入边集合，shard中边按dst排序。

为了提高GPU利用率，允许每个shard小于共享内存（以能放入）。每个shard中的节点数为$C_{shm}/(N_{Block} * S_{vertex})$，其中$C_{shm}$表示共享内存大小，$N_{Block}$表示SM中线程数量，$S_{vertex}$表示节点的大小。若有48KB共享内存，$S_{vertex}$大于32位（以用于十亿规模结点的图）。那么每个分片最多可以包含12K个顶点。由此，我们可以用16位整数表示目标顶点的索引，这样可以节省显存。

为了提高传输速率，可以一次传输多个shard。一次传输的shard集合称为page，希望page可以在GPU中被连续最大存储（多一个shard就存不下）。设$N_s$为流的数量，则shard数目选择，应使得对应的page可以最大连续存储于$1/N_s$的显存中。

系统会采用两种节点中心计算模型，一种是pull，每个顶点拉起去邻近节点状态更新自身；另一种为notify-push（通知-拉取），活跃顶点通知邻居更新状态，邻居再拉取更新自身。CSR只用于通知，Garaph不会在CSR中存储边的值。

![Garaph Architecture](./pic/2017%20Garaph/0%20Garaph%20Architecture.png "Garaph Architecture")

Garaph架构如上图所示，由三个主要部分：调度器 dispatcher，CPU计算内核和GPU计算内核。

* **调度器**  
该模块负责从二级存储中加载图数据，在内存中构建page，并将计算任务分配到CPU和GPU上。
* **GPU/CPU计算内核**  
接收到页面后，GPU计算内核会并行处理page的shard，每个shard由一个block处理。为提高效率，GPU端只会使用pull模型（notify-push会导致warp发散）。  
CPU计算内核会将page的边划分为大小相等的集合，每个线程处理一个集合。CPU与GPU之间，任有一个处理完一个page，就会进行一次CPU/GPU同步。

Garaph支持以同步或异步的方式执行。称处理完所有页面为一次迭代。同步模型中，每次迭代结束后对顶点数据的更新会被提交，下一次迭代可见；而异步则更改会立即被提交。

容错性考虑，Garagh会定期将顶点写入二级存储。同步运行，每隔n次迭代（用户定义）写入；异步运行有两种处理策略，一是每处理完一个页面，就会将更新的顶点数据写入稳定存储；二是每隔n次迭代将所有顶点数据写入稳定存储。发生故障，系统会从二级存储中加载顶点数据继续运行程序。

Garaph实现了GAS(Gather-Apply-Scatter)抽象。若顶点的值发生显著变化，会激活节点。

## GPU图处理

### 图处理引擎

![GPU引擎](./pic/2017%20Garaph/1%20GPU引擎.png "GPU引擎")

GPU在全局内存会维护数组GlobalVertices，该数组存放顶点的值。若有24G全局内存，顶点大小为4字节，则可以存放多大60亿个节点，对于大多数数据集而言是足够的。

shard交给线程块并行处理。当一个page完成计算时，GPU的全局内存和主机内存会同步新的顶点值。现在假设$S_i$为一个目标顶点；
* **初始化**  
GPU在对应SM共享内存中分配数组LocalVertices，里面存储shard中每个顶点的累积值（之后会介绍怎么计算）。线程块中线程使用用户定义的默认顶点值初始化该数组。
* **Gather**    
一个线程处理一条边，其从全局内存获取顶点和变得数据，并增加累积值：$a_u \leftarrow sum(a_u, gather(D_u, D_{(u, v)}, D_v), D_v), v\in Nbr[u]$。为了实现全局内存访问的合并，线程块中连续线程读取全局内存中连续的边数据。
* **Apply**   
每个线程更新共享内存中的顶点值。异步执行时，系统会直接将新的顶点数据提交到GlobalVertices中。
* **GPU到CPU同步**   
一个page处理完毕时，GPU全局内存会和CPU同步。对于异步，GPU全局内存中GlobalVertices的更新值会传输到CPU中存储最新值的数组；对于同步，更新节点时值会存储于GPU临时空间，page计算完后被传输到CPU临时存储空间，在迭代结束后提交。由于PCIe总线是全双工的，且大多数GPU有两个复制引擎，因此同步过程可以与GPU中page处理过程重叠在一起。

### 基于复制的Gather

在Gather阶段，多个边的dst会出现相同的状况，由此会存在写冲突。本文中减少冲突的策略为复制，即对于节点v，在共享内存中存放给R个副本，这R个副本位置连续，R被称为复制因子。  

我们需要将线程分配到不同的副本上，由此需要一个映射公式。设同一个shard $S_i$的顶点具有相同的复制因子$R_i$，对于该分片中任何顶点$u_i$，有

$$addr[u_i^{'}] = (i-r_s)*R_i + tid \% R_i, \forall u_i \in S_i$$

$addr[u_i^{'}]$表示的是对于tid，若它要写入的节点为$u_i$，它所要写入的共享内存的位置。其中$r_s$是$S_i$在共享内存所拥有存储空间的起始索引，可以看出每R个位置归属于一个节点，线程对应于R个位置的哪个，取决于$tid \% R_i$。该过程如下图(a)所示。

![复制节点](./pic/2017%20Garaph/2%20复制节点.png "复制节点")

处理完所有边后，系统需要格外的聚合阶段来聚合所有副本的值。聚合过程如上图(b)所示。由此，对于R的设置，希望$R=2^n$。




