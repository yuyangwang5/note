# Graphie: Large-Scale Asynchronous Graph Traversals on Just a GPU

## 通过顶点重命名优化内核执行

![顶点重命名](./pic/2017%20Graphie/01%20顶点重命名.png "顶点重命名")

### 提高GPU内存访问效率

一种提高内存访问效率的方法时按源顶点ID对边进行排序，排序后可以将源顶点加载到主内存中，减少对主内存的访问。然而，最后一个顶点ID和第一个顶点ID之差可能大于可加载入共享内存的顶点数目。如上图(a)中，第二个分区有3个不同顶点，但差距为4，即实际仅需加载3个顶点但按差距加载的话会加载5个顶点，由此引入对节点进行重命名，主要就是将源节点ID（即有出度的ID）连续重命名。

进行重命名之后，可以使用共享内存进行计算。在内核执行过程中，每个线程计算一条边。线程会先把源节点的值加载到共享内存中，之后再计算。由于源节点可能有许多出边，这可以减少全局内存的访问次数。GPU处理完成整个图后，会将顶点数组(V A_GPU)传回CPU并存储于数组(V A_CPU)中。

### 高效激活分区

一些图算法计算时有动态边界。有些框架使用长度为|V|的数组来计算激活节点，占用的GPU内存较大。在本框架中，对重命名技术进行进一步改进，即由上图的(b)重命名为(c)，使得确定哪个分区有顶点被激活的成本降低。进一步重命名后，确定激活分区仅需要单一的除法操作。这个重命名操作可看作插入了虚拟节点。以(c)为例，分区2有最大的不同顶点数目（3个），由此每个分区都必须要有与分区2相同的顶点数。在分区1、3、4中，均插入了2个虚拟顶点。

之后展示如何使用该新重命名方式优化frontier的获取。此处计算单位不是一个节点，而是一个分区。由此布尔数组activated大小等于P（分区数目）。在获取源节点数据时，索引应该减去一个偏移量，即前面插入的虚拟节点数目。通过边更新目的节点时，如果update函数返回true，则意味目标顶点被更新且有出边，对应的分区应该被激活，该分区ID可以通过除法直接计算出来。

## 通过异步边流(ASYNCHRONOUS EDGE STREAMING)减少data传输

图遍历内存需求高，但计算密度较低。由此，数据传输可能比GPU上的处理时间更长。现代GPU支持并行命令队列（例如Nvidia GPU的Hyper-Q），这使得内核执行与数据传输是可以重叠的。初始化完顶点数组后，Graghie将剩余的GPU内存划分为K个分区缓冲，K是流的数量。在每个流中，内核调用命令总是跟在数据传输命令之后，以处理传输后的分区。发送到同一命令队列的命令会按顺序执行。当前Nvidia GPU最多支持32个命令队列，由此Graphie默认使用32个流。

Graphie确保仅传输激活的分区，但是如果激活的分区已经在GPU内存内了，可能发生冗余传输。由于命令队列为FIFO，关键是在处理它们之前不要覆盖掉他们。由此，Graphie会优先处理已经激活且驻留于GPU内存中的分区来解决这个问题。对于这样的分区，Graphie仅会将内核调用命令插入到上一轮迭代中处理该分区的队列中。

kernel = SourceModule(...)#利用pycuda编写cuda算子
func = kernel.get_function(函数名) #获取pycuda编写的算子
segment_csr(...) #torch_scatter算子
func(...) #运行函数。会报错pycuda._driver.LogicError: cuFuncSetBlockShape failed: invalid resource handle